{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, BatchNormalization, Dropout, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate base data\n",
    "def generate_inventory_data(num_records=1000):\n",
    "    # Define possible values\n",
    "    warehouse_ids = list(range(1, 6))  # 5 warehouses\n",
    "    item_types = ['row', 'product']\n",
    "    region_ids = list(range(1, 5))  # 4 regions\n",
    "    \n",
    "    # Generate random combinations\n",
    "    data = {\n",
    "        'timestamp': [],\n",
    "        'warehouseid': [],\n",
    "        'itemtype': [],\n",
    "        'itemid': [],\n",
    "        'regionid': [],\n",
    "        'currentstock': []\n",
    "    }\n",
    "    \n",
    "    # Create base patterns for each item\n",
    "    num_items = 50\n",
    "    base_stocks = {i: np.random.randint(50, 200) for i in range(1, num_items + 1)}\n",
    "    \n",
    "    # Generate time series data\n",
    "    start_date = datetime(2023, 1, 1)\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        # Generate timestamp with daily frequency\n",
    "        current_date = start_date + timedelta(days=i % 365)\n",
    "        \n",
    "        # Generate random warehouse and item information\n",
    "        warehouse_id = np.random.choice(warehouse_ids)\n",
    "        item_type = np.random.choice(item_types)\n",
    "        item_id = np.random.randint(1, num_items + 1)\n",
    "        region_id = np.random.choice(region_ids)\n",
    "        \n",
    "        # Generate stock levels with patterns\n",
    "        base_stock = base_stocks[item_id]\n",
    "        \n",
    "        # Add seasonal pattern\n",
    "        seasonal_factor = 1 + 0.3 * np.sin(2 * np.pi * (current_date.timetuple().tm_yday / 365))\n",
    "        \n",
    "        # Add trend\n",
    "        trend = 0.1 * (i % 365) / 365\n",
    "        \n",
    "        # Add random variation\n",
    "        noise = np.random.normal(0, 0.1)\n",
    "        \n",
    "        # Calculate final stock\n",
    "        stock = int(max(0, base_stock * seasonal_factor * (1 + trend) * (1 + noise)))\n",
    "        \n",
    "        # Append to data dictionary\n",
    "        data['timestamp'].append(current_date)\n",
    "        data['warehouseid'].append(warehouse_id)\n",
    "        data['itemtype'].append(item_type)\n",
    "        data['itemid'].append(item_id)\n",
    "        data['regionid'].append(region_id)\n",
    "        data['currentstock'].append(stock)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Sort by timestamp and warehouse/item information\n",
    "    df = df.sort_values(['timestamp', 'warehouseid', 'itemid'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "df = generate_inventory_data(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some patterns and relationships\n",
    "def add_patterns(df):\n",
    "    # Add weekend effect (lower stock on weekends)\n",
    "    df['is_weekend'] = df['timestamp'].dt.weekday >= 5\n",
    "    df.loc[df['is_weekend'], 'currentstock'] = df.loc[df['is_weekend'], 'currentstock'] * 0.9\n",
    "    \n",
    "    # Add regional patterns\n",
    "    region_multipliers = {1: 1.2, 2: 0.9, 3: 1.1, 4: 0.8}\n",
    "    for region, multiplier in region_multipliers.items():\n",
    "        df.loc[df['regionid'] == region, 'currentstock'] = \\\n",
    "            df.loc[df['regionid'] == region, 'currentstock'] * multiplier\n",
    "    \n",
    "    # Round stock values to integers\n",
    "    df['currentstock'] = df['currentstock'].astype(int)\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    df = df.drop('is_weekend', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply patterns and save final dataset\n",
    "final_df = add_patterns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to prepare data for LSTM\n",
    "def prepare_lstm_data(df, sequence_length=7):\n",
    "    # Create sequences of data\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # Group by warehouse and item\n",
    "    for (warehouse, item), group in df.groupby(['warehouseid', 'itemid']):\n",
    "        stock_values = group['currentstock'].values\n",
    "        \n",
    "        for i in range(len(stock_values) - sequence_length):\n",
    "            sequences.append(stock_values[i:i+sequence_length])\n",
    "            targets.append(stock_values[i+sequence_length])\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (5000, 6)\n",
      "\n",
      "Sample of the generated data:\n",
      "      timestamp  warehouseid itemtype  itemid  regionid  currentstock\n",
      "730  2023-01-01            1      row      48         3           166\n",
      "0    2023-01-01            1  product      50         4            57\n",
      "2555 2023-01-01            2  product       5         3           111\n",
      "3285 2023-01-01            2  product       9         1           159\n",
      "4015 2023-01-01            2      row      20         1           210\n",
      "\n",
      "Data statistics:\n",
      "                 timestamp  warehouseid       itemid    regionid  currentstock\n",
      "count                 5000  5000.000000  5000.000000  5000.00000   5000.000000\n",
      "mean   2023-06-29 04:40:48     2.978600    25.571400     2.50600    124.004200\n",
      "min    2023-01-01 00:00:00     1.000000     1.000000     1.00000     28.000000\n",
      "25%    2023-03-31 00:00:00     2.000000    13.000000     1.00000     82.000000\n",
      "50%    2023-06-28 00:00:00     3.000000    26.000000     3.00000    117.000000\n",
      "75%    2023-09-26 00:00:00     4.000000    38.000000     4.00000    158.000000\n",
      "max    2023-12-31 00:00:00     5.000000    50.000000     4.00000    327.000000\n",
      "std                    NaN     1.414405    14.349721     1.12384     54.041079\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "X, y = prepare_lstm_data(final_df)\n",
    "\n",
    "# Save to CSV\n",
    "final_df.to_csv('warehouse_inventory_data.csv', index=False)\n",
    "\n",
    "print(\"Dataset shape:\", final_df.shape)\n",
    "print(\"\\nSample of the generated data:\")\n",
    "print(final_df.head())\n",
    "print(\"\\nData statistics:\")\n",
    "print(final_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preparation with Chronological Order and Embeddings\n",
    "def prepare_data(file_path, sequence_length=7):\n",
    "    \"\"\"\n",
    "    Reads the CSV, extracts time features, encodes categorical variables (without scaling),\n",
    "    scales continuous time features, and creates sequences.\n",
    "    \n",
    "    Categorical features: warehouseid, itemtype, itemid, regionid\n",
    "    Continuous features: day_of_week, month\n",
    "    \"\"\"\n",
    "    # Read and sort by time\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values('timestamp')\n",
    "    \n",
    "    # Extract additional time features\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek  # 0 (Monday) to 6 (Sunday)\n",
    "    df['month'] = df['timestamp'].dt.month           # 1 to 12\n",
    "\n",
    "    # Define which columns are categorical vs. continuous\n",
    "    categorical_cols = ['warehouseid', 'itemtype', 'itemid', 'regionid']\n",
    "    continuous_cols = ['day_of_week', 'month']\n",
    "    \n",
    "    # Label-encode categorical variables and capture vocabulary sizes for embeddings\n",
    "    cat_vocab_sizes = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        cat_vocab_sizes[col] = df[col].nunique()\n",
    "    \n",
    "    # Scale continuous features\n",
    "    scaler = MinMaxScaler()\n",
    "    df[continuous_cols] = scaler.fit_transform(df[continuous_cols])\n",
    "    \n",
    "    # Create sequences – here we group by keys that define a unique time series.\n",
    "    # (Adjust group_keys as per your domain; here we assume warehouseid & itemid identify a series.)\n",
    "    X_cat, X_cont, y = [], [], []\n",
    "    group_keys = ['warehouseid', 'itemid']\n",
    "    \n",
    "    for _, group in df.groupby(group_keys):\n",
    "        group = group.sort_values('timestamp')\n",
    "        # Extract values for categorical and continuous features\n",
    "        cat_data = group[categorical_cols].values  # shape: (group_length, num_cat)\n",
    "        cont_data = group[continuous_cols].values    # shape: (group_length, num_cont)\n",
    "        target = group['currentstock'].values\n",
    "        \n",
    "        # Build sliding windows\n",
    "        for i in range(len(group) - sequence_length):\n",
    "            X_cat.append(cat_data[i:i+sequence_length])\n",
    "            X_cont.append(cont_data[i:i+sequence_length])\n",
    "            y.append(target[i+sequence_length])\n",
    "    \n",
    "    X_cat = np.array(X_cat)   # shape: (samples, sequence_length, num_cat)\n",
    "    X_cont = np.array(X_cont) # shape: (samples, sequence_length, num_cont)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X_cat, X_cont, y, cat_vocab_sizes, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Chronological Train/Validation/Test Split\n",
    "def split_data(X_cat, X_cont, y, train_ratio=0.6, val_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits the data chronologically into train, validation, and test sets.\n",
    "    For example, with 60% training, 20% validation, and 20% testing.\n",
    "    \"\"\"\n",
    "    total = len(y)\n",
    "    train_end = int(total * train_ratio)\n",
    "    val_end = int(total * (train_ratio + val_ratio))\n",
    "    \n",
    "    X_cat_train = X_cat[:train_end]\n",
    "    X_cont_train = X_cont[:train_end]\n",
    "    y_train = y[:train_end]\n",
    "    \n",
    "    X_cat_val = X_cat[train_end:val_end]\n",
    "    X_cont_val = X_cont[train_end:val_end]\n",
    "    y_val = y[train_end:val_end]\n",
    "    \n",
    "    X_cat_test = X_cat[val_end:]\n",
    "    X_cont_test = X_cont[val_end:]\n",
    "    y_test = y[val_end:]\n",
    "    \n",
    "    return (X_cat_train, X_cont_train, y_train), (X_cat_val, X_cont_val, y_val), (X_cat_test, X_cont_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Definition with Embeddings, LSTM Layers, and Regularization\n",
    "def create_model(sequence_length, num_cont_features, cat_vocab_sizes, embedding_dims=None):\n",
    "    \"\"\"\n",
    "    Creates a multi-input LSTM model.\n",
    "    For each categorical feature, an embedding layer is created.\n",
    "    \n",
    "    Args:\n",
    "        sequence_length (int): Number of time steps per sample.\n",
    "        num_cont_features (int): Number of continuous features.\n",
    "        cat_vocab_sizes (dict): Mapping from categorical column name to vocabulary size.\n",
    "        embedding_dims (dict): (Optional) Mapping from categorical column to embedding dimension.\n",
    "                               If not provided, a default rule is used.\n",
    "    \"\"\"\n",
    "    if embedding_dims is None:\n",
    "        embedding_dims = {}\n",
    "        # Simple rule: embedding dimension = min(50, (vocab_size + 1) // 2)\n",
    "        for col, vocab_size in cat_vocab_sizes.items():\n",
    "            embedding_dims[col] = min(50, (vocab_size + 1) // 2)\n",
    "    \n",
    "    categorical_cols = list(cat_vocab_sizes.keys())\n",
    "    \n",
    "    # Define an input for each categorical feature.\n",
    "    cat_inputs = {}\n",
    "    cat_embeddings = []\n",
    "    for col in categorical_cols:\n",
    "        inp = Input(shape=(sequence_length,), name=f'{col}_input')\n",
    "        cat_inputs[col] = inp\n",
    "        vocab_size = cat_vocab_sizes[col]\n",
    "        embed_dim = embedding_dims[col]\n",
    "        # Create an embedding layer; note that mask_zero is left as False assuming 0 is a valid index.\n",
    "        embed = Embedding(input_dim=vocab_size, output_dim=embed_dim, name=f'{col}_embed')(inp)\n",
    "        cat_embeddings.append(embed)\n",
    "    \n",
    "    # Concatenate embeddings along the last dimension.\n",
    "    if len(cat_embeddings) > 1:\n",
    "        cat_concat = Concatenate(name='cat_concat')(cat_embeddings)\n",
    "    else:\n",
    "        cat_concat = cat_embeddings[0]\n",
    "    \n",
    "    # Input layer for continuous features\n",
    "    cont_input = Input(shape=(sequence_length, num_cont_features), name='cont_input')\n",
    "    \n",
    "    # Combine categorical and continuous features along the feature axis\n",
    "    x = Concatenate(name='combined_features')([cat_concat, cont_input])\n",
    "    \n",
    "    # LSTM layers with bidirectionality, dropout, batch normalization, and L2 regularization.\n",
    "    x = Bidirectional(LSTM(128, activation='tanh', return_sequences=True, \n",
    "                           kernel_regularizer=tf.keras.regularizers.l2(1e-4)))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Bidirectional(LSTM(64, activation='tanh', return_sequences=True, \n",
    "                           kernel_regularizer=tf.keras.regularizers.l2(1e-4)))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = LSTM(32, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Fully connected layers for final feature extraction\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    output = Dense(1, name='output')(x)\n",
    "    \n",
    "    # Combine all inputs: categorical inputs (as a list) and continuous input.\n",
    "    inputs = list(cat_inputs.values()) + [cont_input]\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='huber',  # More robust to outliers\n",
    "                  metrics=['mae', 'mse'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Training Function (No Shuffling for Chronology)\n",
    "def train_model(model, train_data, val_data, epochs=50, batch_size=32):\n",
    "    \"\"\"\n",
    "    Trains the model using the provided training and validation data.\n",
    "    Note: `shuffle=False` is critical for chronological data.\n",
    "    \"\"\"\n",
    "    X_cat_train, X_cont_train, y_train = train_data\n",
    "    X_cat_val, X_cont_val, y_val = val_data\n",
    "    \n",
    "    # Prepare the input dictionary. The order of inputs must match the model inputs.\n",
    "    # Here we assume the categorical columns are in the order: \n",
    "    # ['warehouseid', 'itemtype', 'itemid', 'regionid'].\n",
    "    input_train = {\n",
    "        'warehouseid_input': X_cat_train[:, :, 0],\n",
    "        'itemtype_input': X_cat_train[:, :, 1],\n",
    "        'itemid_input': X_cat_train[:, :, 2],\n",
    "        'regionid_input': X_cat_train[:, :, 3],\n",
    "        'cont_input': X_cont_train\n",
    "    }\n",
    "    input_val = {\n",
    "        'warehouseid_input': X_cat_val[:, :, 0],\n",
    "        'itemtype_input': X_cat_val[:, :, 1],\n",
    "        'itemid_input': X_cat_val[:, :, 2],\n",
    "        'regionid_input': X_cat_val[:, :, 3],\n",
    "        'cont_input': X_cont_val\n",
    "    }\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, mode='min'),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, mode='min')\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        input_train, y_train,\n",
    "        validation_data=(input_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        shuffle=False,  # Do not shuffle – we must keep the chronological order!\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " warehouseid_input (InputLayer)  [(None, 7)]         0           []                               \n",
      "                                                                                                  \n",
      " itemtype_input (InputLayer)    [(None, 7)]          0           []                               \n",
      "                                                                                                  \n",
      " itemid_input (InputLayer)      [(None, 7)]          0           []                               \n",
      "                                                                                                  \n",
      " regionid_input (InputLayer)    [(None, 7)]          0           []                               \n",
      "                                                                                                  \n",
      " warehouseid_embed (Embedding)  (None, 7, 3)         15          ['warehouseid_input[0][0]']      \n",
      "                                                                                                  \n",
      " itemtype_embed (Embedding)     (None, 7, 1)         2           ['itemtype_input[0][0]']         \n",
      "                                                                                                  \n",
      " itemid_embed (Embedding)       (None, 7, 25)        1250        ['itemid_input[0][0]']           \n",
      "                                                                                                  \n",
      " regionid_embed (Embedding)     (None, 7, 2)         8           ['regionid_input[0][0]']         \n",
      "                                                                                                  \n",
      " cat_concat (Concatenate)       (None, 7, 31)        0           ['warehouseid_embed[0][0]',      \n",
      "                                                                  'itemtype_embed[0][0]',         \n",
      "                                                                  'itemid_embed[0][0]',           \n",
      "                                                                  'regionid_embed[0][0]']         \n",
      "                                                                                                  \n",
      " cont_input (InputLayer)        [(None, 7, 2)]       0           []                               \n",
      "                                                                                                  \n",
      " combined_features (Concatenate  (None, 7, 33)       0           ['cat_concat[0][0]',             \n",
      " )                                                                'cont_input[0][0]']             \n",
      "                                                                                                  \n",
      " bidirectional_8 (Bidirectional  (None, 7, 256)      165888      ['combined_features[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 7, 256)      1024        ['bidirectional_8[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " bidirectional_9 (Bidirectional  (None, 7, 128)      164352      ['batch_normalization_20[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 7, 128)      512         ['bidirectional_9[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 7, 128)       0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_14 (LSTM)                 (None, 32)           20608       ['dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 32)          128         ['lstm_14[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 64)           2112        ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 64)          256         ['dense_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 64)           0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 32)           2080        ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 32)          128         ['dense_12[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 32)           0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            33          ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 358,396\n",
      "Trainable params: 357,372\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 71s 373ms/step - loss: 114.7736 - mae: 115.1888 - mse: 15824.1846 - val_loss: 113.4425 - val_mae: 113.8583 - val_mse: 15305.5312 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 14s 222ms/step - loss: 114.1309 - mae: 114.5473 - mse: 15694.7100 - val_loss: 112.6090 - val_mae: 113.0260 - val_mse: 15117.2354 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 12s 196ms/step - loss: 113.0455 - mae: 113.4628 - mse: 15418.8213 - val_loss: 111.2695 - val_mae: 111.6869 - val_mse: 14826.4141 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 111.3600 - mae: 111.7774 - mse: 15075.3291 - val_loss: 109.9130 - val_mae: 110.3305 - val_mse: 14537.0527 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 10s 158ms/step - loss: 108.9761 - mae: 109.3934 - mse: 14549.3867 - val_loss: 108.0173 - val_mae: 108.4343 - val_mse: 14132.9199 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 11s 173ms/step - loss: 105.9936 - mae: 106.4104 - mse: 13898.7891 - val_loss: 103.0992 - val_mae: 103.5156 - val_mse: 13099.3135 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 10s 171ms/step - loss: 102.4560 - mae: 102.8721 - mse: 13133.5215 - val_loss: 102.2282 - val_mae: 102.6438 - val_mse: 12859.2139 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 11s 189ms/step - loss: 98.1309 - mae: 98.5461 - mse: 12228.1826 - val_loss: 98.6937 - val_mae: 99.1087 - val_mse: 12159.1826 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 10s 169ms/step - loss: 93.3088 - mae: 93.7235 - mse: 11291.2949 - val_loss: 84.3170 - val_mae: 84.7316 - val_mse: 9460.9316 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 10s 160ms/step - loss: 87.9378 - mae: 88.3520 - mse: 10326.1211 - val_loss: 84.3545 - val_mae: 84.7687 - val_mse: 9479.2646 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 11s 182ms/step - loss: 81.8742 - mae: 82.2876 - mse: 9290.0264 - val_loss: 78.6963 - val_mae: 79.1095 - val_mse: 8563.6572 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 11s 174ms/step - loss: 75.7324 - mae: 76.1439 - mse: 8280.3633 - val_loss: 67.6428 - val_mae: 68.0522 - val_mse: 6764.3018 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 10s 166ms/step - loss: 69.3320 - mae: 69.7422 - mse: 7162.2104 - val_loss: 62.2731 - val_mae: 62.6808 - val_mse: 5862.5928 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 10s 158ms/step - loss: 62.6583 - mae: 63.0652 - mse: 6166.8057 - val_loss: 50.1503 - val_mae: 50.5542 - val_mse: 4168.9009 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 8s 134ms/step - loss: 55.5045 - mae: 55.9049 - mse: 5136.2212 - val_loss: 40.4098 - val_mae: 40.8064 - val_mse: 3032.5540 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 8s 127ms/step - loss: 49.1689 - mae: 49.5649 - mse: 4239.1177 - val_loss: 37.7727 - val_mae: 38.1667 - val_mse: 2578.1865 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 7s 122ms/step - loss: 43.3908 - mae: 43.7827 - mse: 3431.9907 - val_loss: 40.0478 - val_mae: 40.4382 - val_mse: 2889.0020 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 8s 133ms/step - loss: 38.6674 - mae: 39.0562 - mse: 2836.2622 - val_loss: 39.0461 - val_mae: 39.4361 - val_mse: 2741.3972 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 8s 133ms/step - loss: 35.5125 - mae: 35.8990 - mse: 2456.2771 - val_loss: 32.2216 - val_mae: 32.6080 - val_mse: 1863.1002 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 7s 114ms/step - loss: 33.9259 - mae: 34.3104 - mse: 2225.5769 - val_loss: 30.2172 - val_mae: 30.5982 - val_mse: 1700.6387 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 6s 106ms/step - loss: 32.4055 - mae: 32.7882 - mse: 2005.1171 - val_loss: 28.7359 - val_mae: 29.1174 - val_mse: 1429.0555 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 6s 105ms/step - loss: 31.7315 - mae: 32.1131 - mse: 1876.2225 - val_loss: 31.0451 - val_mae: 31.4253 - val_mse: 1677.9302 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 8s 128ms/step - loss: 30.8710 - mae: 31.2496 - mse: 1779.9200 - val_loss: 27.0346 - val_mae: 27.4118 - val_mse: 1310.6615 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 8s 125ms/step - loss: 29.8471 - mae: 30.2241 - mse: 1651.0602 - val_loss: 26.6460 - val_mae: 27.0207 - val_mse: 1354.2942 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 7s 120ms/step - loss: 30.2540 - mae: 30.6300 - mse: 1702.9226 - val_loss: 25.9277 - val_mae: 26.2993 - val_mse: 1159.1449 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 29.0039 - mae: 29.3773 - mse: 1517.7144 - val_loss: 24.7907 - val_mae: 25.1623 - val_mse: 1101.5927 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 8s 123ms/step - loss: 29.0905 - mae: 29.4610 - mse: 1528.8341 - val_loss: 25.0425 - val_mae: 25.4135 - val_mse: 1095.6199 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 28.3211 - mae: 28.6921 - mse: 1438.4534 - val_loss: 25.2209 - val_mae: 25.5911 - val_mse: 1082.0563 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 28.2699 - mae: 28.6378 - mse: 1445.1576 - val_loss: 25.4921 - val_mae: 25.8605 - val_mse: 1100.5469 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 27.5986 - mae: 27.9650 - mse: 1401.7876 - val_loss: 24.8661 - val_mae: 25.2319 - val_mse: 1099.0577 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 11s 178ms/step - loss: 28.1246 - mae: 28.4907 - mse: 1422.2738 - val_loss: 25.5645 - val_mae: 25.9310 - val_mse: 1160.0143 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 27.3740 - mae: 27.7387 - mse: 1378.4730 - val_loss: 23.7725 - val_mae: 24.1351 - val_mse: 1008.5792 - lr: 5.0000e-04\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 26.9433 - mae: 27.3073 - mse: 1336.2911 - val_loss: 23.1814 - val_mae: 23.5470 - val_mse: 971.2472 - lr: 5.0000e-04\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 7s 120ms/step - loss: 26.4939 - mae: 26.8564 - mse: 1327.8374 - val_loss: 23.7168 - val_mae: 24.0823 - val_mse: 987.9490 - lr: 5.0000e-04\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 7s 115ms/step - loss: 26.5544 - mae: 26.9192 - mse: 1291.1158 - val_loss: 23.5780 - val_mae: 23.9400 - val_mse: 994.5532 - lr: 5.0000e-04\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 7s 110ms/step - loss: 26.4791 - mae: 26.8420 - mse: 1302.7659 - val_loss: 24.0748 - val_mae: 24.4367 - val_mse: 1046.4042 - lr: 5.0000e-04\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 26.1289 - mae: 26.4915 - mse: 1283.1268 - val_loss: 23.6437 - val_mae: 24.0052 - val_mse: 993.3355 - lr: 5.0000e-04\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 25.8005 - mae: 26.1624 - mse: 1238.9637 - val_loss: 24.2306 - val_mae: 24.5946 - val_mse: 1050.9380 - lr: 5.0000e-04\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 25.5884 - mae: 25.9501 - mse: 1228.7267 - val_loss: 23.8176 - val_mae: 24.1812 - val_mse: 1011.6582 - lr: 2.5000e-04\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 47s 785ms/step - loss: 25.6845 - mae: 26.0464 - mse: 1248.9949 - val_loss: 23.5660 - val_mae: 23.9281 - val_mse: 1003.9620 - lr: 2.5000e-04\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 25.3445 - mae: 25.7060 - mse: 1226.2139 - val_loss: 23.7731 - val_mae: 24.1348 - val_mse: 1026.3949 - lr: 2.5000e-04\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 8s 127ms/step - loss: 25.1275 - mae: 25.4896 - mse: 1185.7222 - val_loss: 23.9600 - val_mae: 24.3208 - val_mse: 1049.7909 - lr: 2.5000e-04\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 7s 108ms/step - loss: 25.3693 - mae: 25.7292 - mse: 1218.5475 - val_loss: 23.6256 - val_mae: 23.9881 - val_mse: 1015.5715 - lr: 2.5000e-04\n",
      "\n",
      "Test Loss: 24.49, Test MAE: 24.86, Test MSE: 1104.20\n",
      "21/21 [==============================] - 5s 23ms/step\n",
      "Predictions shape: (650, 1)\n"
     ]
    }
   ],
   "source": [
    "# 5. Main Execution: Data Prep, Splitting, Model Creation, Training, and Evaluation\n",
    "sequence_length = 7  # Number of past time steps used for prediction\n",
    "file_path = 'data/warehouse_inventory_data.csv'\n",
    "    \n",
    "# Prepare the data\n",
    "X_cat, X_cont, y, cat_vocab_sizes, cont_scaler = prepare_data(file_path, sequence_length)\n",
    "    \n",
    "# Chronologically split the data into train (60%), validation (20%), and test (20%) sets.\n",
    "train_data, val_data, test_data = split_data(X_cat, X_cont, y, train_ratio=0.6, val_ratio=0.2)\n",
    "    \n",
    "# Build the model.\n",
    "num_cont_features = X_cont.shape[2]  # e.g., 2 for day_of_week and month.\n",
    "model = create_model(sequence_length, num_cont_features, cat_vocab_sizes)\n",
    "model.summary()\n",
    "    \n",
    "# Train the model (no shuffling for time series!)\n",
    "history = train_model(model, train_data, val_data, epochs=50, batch_size=32)\n",
    "    \n",
    "# Prepare test input dictionary\n",
    "X_cat_test, X_cont_test, y_test = test_data\n",
    "test_inputs = {\n",
    "    'warehouseid_input': X_cat_test[:, :, 0],\n",
    "    'itemtype_input': X_cat_test[:, :, 1],\n",
    "    'itemid_input': X_cat_test[:, :, 2],\n",
    "    'regionid_input': X_cat_test[:, :, 3],\n",
    "    'cont_input': X_cont_test\n",
    "}\n",
    "    \n",
    "# Evaluate the model on the test set.\n",
    "test_loss, test_mae, test_mse = model.evaluate(test_inputs, y_test, verbose=0)\n",
    "print(f\"\\nTest Loss: {test_loss:.2f}, Test MAE: {test_mae:.2f}, Test MSE: {test_mse:.2f}\")\n",
    "    \n",
    "# Make predictions on the test set.\n",
    "predictions = model.predict(test_inputs)\n",
    "print(\"Predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_cat_new: (9, 7, 4)\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 71.2700 - mae: 71.6027 - mse: 8216.3867\n",
      "\n",
      "New Data - Loss: 71.27, MAE: 71.60, MSE: 8216.39\n",
      "1/1 [==============================] - 0s 250ms/step\n",
      "Predictions shape on new data: (9, 1)\n"
     ]
    }
   ],
   "source": [
    "# --- Testing on a new dataset ---\n",
    "# If you have a separate CSV for new data, process it with the same function.\n",
    "new_file_path = 'data/test_data_inventory.csv'  # Update this path to your new data file\n",
    "# Process new data\n",
    "X_cat_new, X_cont_new, y_new, _, _ = prepare_data(new_file_path, sequence_length)\n",
    "\n",
    "# Debug: print the shape of X_cat_new\n",
    "print(\"Shape of X_cat_new:\", X_cat_new.shape)\n",
    "\n",
    "# Check if the new data produced valid sequences\n",
    "if X_cat_new.ndim != 3 or X_cat_new.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in the new CSV file to create sequences. \"\n",
    "                     \"Ensure that the file has enough rows for at least one sequence of length \"\n",
    "                     f\"{sequence_length} for each group.\")\n",
    "\n",
    "# Build input dictionary for the new test data.\n",
    "new_test_inputs = {\n",
    "    'warehouseid_input': X_cat_new[:, :, 0],\n",
    "    'itemtype_input': X_cat_new[:, :, 1],\n",
    "    'itemid_input': X_cat_new[:, :, 2],\n",
    "    'regionid_input': X_cat_new[:, :, 3],\n",
    "    'cont_input': X_cont_new\n",
    "}\n",
    "\n",
    "# Evaluate the model on the new data.\n",
    "new_loss, new_mae, new_mse = model.evaluate(new_test_inputs, y_new, verbose=1)\n",
    "print(f\"\\nNew Data - Loss: {new_loss:.2f}, MAE: {new_mae:.2f}, MSE: {new_mse:.2f}\")\n",
    "\n",
    "# Make predictions on the new data.\n",
    "new_predictions = model.predict(new_test_inputs)\n",
    "print(\"Predictions shape on new data:\", new_predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[7.50000000e-01, 1.00000000e+00, 6.66666667e-01, 6.66666667e-01,\n",
       "         2.72727273e-01, 1.25000000e+02],\n",
       "        [7.50000000e-01, 1.00000000e+00, 6.66666667e-01, 0.00000000e+00,\n",
       "         3.63636364e-01, 1.06000000e+02],\n",
       "        [7.50000000e-01, 1.00000000e+00, 3.33333333e-01, 3.33333333e-01,\n",
       "         4.54545455e-01, 9.20000000e+01],\n",
       "        ...,\n",
       "        [7.50000000e-01, 1.00000000e+00, 6.66666667e-01, 8.33333333e-01,\n",
       "         9.09090909e-01, 6.60000000e+01],\n",
       "        [7.50000000e-01, 1.00000000e+00, 3.33333333e-01, 6.66666667e-01,\n",
       "         9.09090909e-01, 6.70000000e+01],\n",
       "        [7.50000000e-01, 1.00000000e+00, 1.00000000e+00, 6.66666667e-01,\n",
       "         1.00000000e+00, 6.40000000e+01]],\n",
       "\n",
       "       [[1.00000000e+00, 0.00000000e+00, 3.33333333e-01, 0.00000000e+00,\n",
       "         0.00000000e+00, 1.44000000e+02],\n",
       "        [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 8.33333333e-01,\n",
       "         0.00000000e+00, 1.35000000e+02],\n",
       "        [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.33333333e-01,\n",
       "         9.09090909e-02, 1.81000000e+02],\n",
       "        ...,\n",
       "        [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.00000000e-01,\n",
       "         3.63636364e-01, 2.01000000e+02],\n",
       "        [1.00000000e+00, 0.00000000e+00, 6.66666667e-01, 5.00000000e-01,\n",
       "         3.63636364e-01, 2.32000000e+02],\n",
       "        [1.00000000e+00, 0.00000000e+00, 6.66666667e-01, 3.33333333e-01,\n",
       "         5.45454545e-01, 1.39000000e+02]],\n",
       "\n",
       "       [[1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 8.33333333e-01,\n",
       "         0.00000000e+00, 1.35000000e+02],\n",
       "        [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.33333333e-01,\n",
       "         9.09090909e-02, 1.81000000e+02],\n",
       "        [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "         2.72727273e-01, 1.57000000e+02],\n",
       "        ...,\n",
       "        [1.00000000e+00, 0.00000000e+00, 6.66666667e-01, 5.00000000e-01,\n",
       "         3.63636364e-01, 2.32000000e+02],\n",
       "        [1.00000000e+00, 0.00000000e+00, 6.66666667e-01, 3.33333333e-01,\n",
       "         5.45454545e-01, 1.39000000e+02],\n",
       "        [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 8.33333333e-01,\n",
       "         5.45454545e-01, 9.40000000e+01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.00000000e+00, 1.00000000e+00, 3.33333333e-01, 0.00000000e+00,\n",
       "         3.63636364e-01, 1.05000000e+02],\n",
       "        [1.00000000e+00, 1.00000000e+00, 3.33333333e-01, 8.33333333e-01,\n",
       "         4.54545455e-01, 7.30000000e+01],\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.66666667e-01,\n",
       "         4.54545455e-01, 7.70000000e+01],\n",
       "        ...,\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "         5.45454545e-01, 6.10000000e+01],\n",
       "        [1.00000000e+00, 1.00000000e+00, 0.00000000e+00, 3.33333333e-01,\n",
       "         5.45454545e-01, 9.60000000e+01],\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "         6.36363636e-01, 6.20000000e+01]],\n",
       "\n",
       "       [[1.00000000e+00, 1.00000000e+00, 3.33333333e-01, 8.33333333e-01,\n",
       "         4.54545455e-01, 7.30000000e+01],\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.66666667e-01,\n",
       "         4.54545455e-01, 7.70000000e+01],\n",
       "        [1.00000000e+00, 1.00000000e+00, 6.66666667e-01, 6.66666667e-01,\n",
       "         4.54545455e-01, 9.00000000e+01],\n",
       "        ...,\n",
       "        [1.00000000e+00, 1.00000000e+00, 0.00000000e+00, 3.33333333e-01,\n",
       "         5.45454545e-01, 9.60000000e+01],\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "         6.36363636e-01, 6.20000000e+01],\n",
       "        [1.00000000e+00, 1.00000000e+00, 6.66666667e-01, 8.33333333e-01,\n",
       "         7.27272727e-01, 6.70000000e+01]],\n",
       "\n",
       "       [[1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.66666667e-01,\n",
       "         4.54545455e-01, 7.70000000e+01],\n",
       "        [1.00000000e+00, 1.00000000e+00, 6.66666667e-01, 6.66666667e-01,\n",
       "         4.54545455e-01, 9.00000000e+01],\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "         5.45454545e-01, 6.10000000e+01],\n",
       "        ...,\n",
       "        [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "         6.36363636e-01, 6.20000000e+01],\n",
       "        [1.00000000e+00, 1.00000000e+00, 6.66666667e-01, 8.33333333e-01,\n",
       "         7.27272727e-01, 6.70000000e+01],\n",
       "        [1.00000000e+00, 1.00000000e+00, 3.33333333e-01, 5.00000000e-01,\n",
       "         1.00000000e+00, 7.80000000e+01]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_sequence = X_test.copy()\n",
    "current_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_future_stock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example prediction\u001b[39;00m\n\u001b[0;32m      2\u001b[0m sample_sequence \u001b[38;5;241m=\u001b[39m X_test[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m future_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_future_stock\u001b[49m(model, sample_sequence)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNext 7 days stock predictions for a sample item:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(future_predictions)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict_future_stock' is not defined"
     ]
    }
   ],
   "source": [
    "# Example prediction\n",
    "sample_sequence = X_test[0]\n",
    "future_predictions = predict_future_stock(model, sample_sequence)\n",
    "print(\"\\nNext 7 days stock predictions for a sample item:\")\n",
    "print(future_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synexflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
